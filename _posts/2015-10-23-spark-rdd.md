---
layout: post
title:  "Spark : Resilient Distributed Dataset"
date:   2015-10-23 7:40:22
categories: Spark
author : Jithesh Chandrasekharan
image: 
comments: true
---

RDD's are spark's core abstraction for working with data. A RDD is simply a distributed collection of elements. In Spark, all work/job is expressed as either creating a new RDD, transforming existing RDDs, or calling operations on RDD to compute a result. Under the hood, Spark distributes the data contained in the RDDs across your cluster and parallelizes the operations you perform on them.

An RDD in Spark is simply an immutable distributed collection of objects. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes. 

A usual programming paradigm in spark is as following.

{% highlight python %}
Create an RDD 
	lines = sc.textFile("README.md")

Apply Transformations on RDD
	py_lines = lines.filter(lambda line: "Python" in line)

Persist the RDD for future computation (Optional)
	py_lines.persist()

Apply Actions on RDD to get the result.
	print py_lines.count()

{% endhighlight %}

**Create an RDD:**

Users create RDDs in two ways: by loading an external dataset, or by distributing a collection of objects (e.g., a list or set) in their driver program. We have already seen loading a text file as an RDD of strings using SparkContext.textFile()

*lines = sc.textFile("README.md")*

or

*lines = sc.parallelize(["python", "i like python"])*


**RDD Transformations:**

Once created, RDDs offer two types of operations: transformations and actions. Transformations are operations on RDDs that return a new RDD, such as map() and filter(). For example, one common transformation is filtering data that matches a predicate. In our text file example, we used this to create a new RDD holding just the strings that contain the word Python, as shown in Example.

*py_lines = lines.filter(lambda line: "Python" in line)*

Note that the filter() operation does not mutate the existing inputRDD (lines). Instead, it returns a pointer to an entirely new RDD. inputRDD can still be reused later in the program—for instance, to search for other words. In fact, let’s use inputRDD again to search for lines with the word 'java' in them. Then, we’ll use another transformation, union(), to print out the number of lines that contained either 'Python' or 'Java'.

{% highlight python %}

from pyspark import SparkConf , SparkContext
import collections

conf = SparkConf().setMaster("local").setAppName("pylinefinder")
sc = SparkContext(conf = conf)

lines = sc.textFile("README.md")

py_lines = lines.filter(lambda line: "Python" in line)

jv_lines = lines.filter(lambda line: "Java" in line)

py_jv = py_lines.union(jv_lines)

print py_jv.count()

{% endhighlight %}

Finally, as you derive new RDDs from each other using transformations, Spark keeps track of the set of dependencies between different RDDs, called the lineage graph. It uses this information to compute each RDD on demand and to recover lost data if part of a persistent RDD is lost. This figure shows lineage graph.

![Spark Download](/img/lineage.png)

**RDD Actions:**

Actions, on the other hand, compute a result based on an RDD, and either return it to the driver program or save it to an external storage system (e.g., HDFS). Actions force the evaluation of the transformations required for the RDD they were called on, since they need to actually produce output. An action we called earlier is count(), which returns the count of number of elements in the RDD.

*print py_lines.count()*

It is important to note that, the transformations are not applied on an RDD immediately, instead they are done only when Spark encounter an Action on that RDD. This is called as Lazy evaluation, which is common in functional programming, which makes sense when dealing with big data. If Spark were to load and store all the lines in the file as soon as we wrote lines = sc.textFile(...), it would waste a lot of storage space, given that we then immediately filter out many lines. Instead, once Spark sees the whole chain of transformations, it can compute just the data needed for its result.

Finally, Spark’s RDDs are by default recomputed each time you run an action on them. If you would like to reuse an RDD in multiple actions, you can ask Spark to persist it using RDD.persist().After computing it the first time, Spark will store the RDD contents in memory (partitioned across the machines in your cluster), and reuse them in future actions.

Persisting RDDs on disk instead of memory is also possible. Spark by default won't persist data in disk, but it makes a lot of sense for big datasets: if you will not reuse the RDD, there’s no reason to waste storage space when Spark could instead stream through the data once and just compute the result.

In practice, you will often use persist() to load a subset of your data into memory and query it repeatedly. For example, if we knew that we wanted to compute multiple results about the README lines that contain Python, we could write the script shown below.

{% highlight python %}
py_lines.persist()
py_lines.count()
{% endhighlight %}

**take() or Collect()**
take() is used to retrieve a small number of elements in the RDD at the driver program. We then iterate over them locally to print out information at the driver. RDDs also have a collect() function to retrieve the entire RDD. This can be useful if your program filters RDDs down to a very small size and you’d like to deal with it locally. Keep in mind that your entire dataset must fit in memory on a single machine to use collect() on it, so collect() shouldn’t be used on large datasets.
In most cases RDDs can’t just be collect()ed to the driver because they are too large. In these cases, it’s common to write data out to a distributed storage system such as HDFS or Amazon S3. You can save the contents of an RDD using the saveAsTextFile() action, saveAsSequenceFile(), or any of a number of actions for various built-in formats. We will cover the different options for exporting data in Chapter 5.
It is important to note that each time we call a new action, the entire RDD must be computed “from scratch.” To avoid this inefficiency, users can persist intermediate results.

**Lazy Evaluations**
Lazy evaluation means that when we call a transformation on an RDD (for instance, calling map()), the operation is not immediately performed. Instead, Spark internally records metadata to indicate that this operation has been requested. Rather than thinking of an RDD as containing specific data, it is best to think of each RDD as consisting of instructions on how to compute the data that we build up through transformations. Loading data into an RDD is lazily evaluated in the same way transformations are. So, when we call sc.textFile(), the data is not loaded until it is necessary. As with transformations, the operation (in this case, reading the data) can occur multiple times.Although transformations are lazy, you can force Spark to execute them at any time by running an action, such as count(). This is an easy way to test out just part of your program.

Spark uses lazy evaluation to reduce the number of passes it has to take over our data by grouping operations together. In systems like Hadoop MapReduce, developers often have to spend a lot of time considering how to group together operations to minimize the number of MapReduce passes. In Spark, there is no substantial benefit to writing a single complex map instead of chaining together many simple operations. Thus, users are free to organize their program into smaller, more manageable operations.